---
layout: page
title: "SVM"
date: 2019-11-12 12:39:12 -0000

categories: 
- Machine Learning
- Math
---
## Hard SVM 
### Math Provement of Margin
Recall: Data $D=\{x^{(i)}, y^{i}\}_{i=1}^N$ is linearly separable iff   

$$\exists\:\vec w, b \:\:s.t. \:\:\:\:\:w^Tx^(i) + b > 0 \;\;if\:y^{(i)}=+1 $$

$$\:\:\:\:\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;w^Tx^(i) + b < 0 \;\;if\:y^{(i)}=-1$$

$$\Longleftrightarrow \:\: \exists\:\vec w, b \:\:s.t. \:\:\:\:\:y^{(i)}(w^Tx^{(i)} + b) > 0$$

$$\Longleftrightarrow \:\: \exists\:\vec w, b, c \:\:s.t. \:\:\:\:\:y^{(i)}(w^Tx^{(i)} + b) \geq c \;and\; c > 0 \:\;\;\;\;A$$

$$\Longleftrightarrow \:\: \exists\:\tilde w, \tilde b \:\:s.t. \:\:\:\:\:y^{(i)}(\tilde w^Tx^{(i)} + \tilde b) \geq 1 \:\;\;\;\;B$$

Trick question:

Is A equal to B?


Yes. 
$\tilde w = \frac{\vec w}{c}$ 
$\tilde b =\frac{b}{c}$ 

### Object Function

* Definition 

$${max}\;\;{margin}_{w, b}$$
$$s.t., y^{(i)}(w^Tx^(i) + b) \geq 1,\forall i$$

* Margin


$$d_+=\frac{w^Tx_+}{\vert\vert w\vert\vert_2} + b$$
$$d_-= -\frac{w^Tx_-}{\vert\vert w\vert\vert_2} - b$$


$d_+$ is the margin from positive side.   
$d_-$ is the margin from negative side. 

So 

$$width = d_+ + d_-$$

$$\;\;\;\;\;\;\;\;\;\ = \frac{w^Tx_+}{||w||_2} + b  -\frac{w^Tx_-}{||w||_2} - b$$

$$\;\;\;\;\;\;\;\;\;\;\;\;\;\; = \frac{1}{||w||_2}((1-b) - (-1 - b))$$

$$ = \frac{2}{||w||_2}$$

Trick point:
$w^Tx_+ +b = 1$, $^Tx_- +b = -1$

3. SVM QP and L2

$$max\;margin {\Longleftrightarrow}\; max\frac{2}{||w||_2}\; {\Longleftrightarrow}\; min\frac{1}{2}||w||_2\;{\Longleftrightarrow}\; min\frac{1}{2}||w||_2^2\;{\Longleftrightarrow} min\frac{1}{2}w^Tw$$


$${min}\;\;\frac{1}{2}w^Tw(L2)$$

$$s.t., y^{(i)}(w^Tx^(i) + b) \geq 1,\forall i$$

## SVM DUAL

### Lagrange Multipliers

Goal: $minf(\vec x) \;\;s.t.\;\; g(\vec x) \leq c$

* Construct Lagrange

$$L(\vec x, \lambda) = f(\vec x ) - \lambda(g(\vec x) - c)$$

* Solve

   $$min_{\vec x}max_{\lambda}L(\vec x,\lambda)$$


   Tip: 
   
   $max_{\lambda}L=f(\vec x), if \vec x\;\;satisfies\;all\;the\;constraints$


   $$\bigtriangledown L(\vec x, \lambda) = 0\;\;s.t.\;\lambda \geq 0, g(\vec x) \leq c$$

   Equivalent to solve:

   $$\bigtriangledown f(\vec x) = \lambda \bigtriangledown g(\vec x) \;\;s.t.\;\lambda \geq 0, g(\vec x) \leq c $$


### SVM with KKT
$$L(\vec w, b, \vec \alpha)=\frac{1}{2}w^Tw - \sum\alpha_i[y^{(i)}(w^Tx^{(i)}+b) - 1]$$

Optimize:

In [Karush–Kuhn–Tucker conditions:](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)


$$min_{w,b}max_{\alpha}L(\vec w, b, \vec \alpha)$$
$$\Longleftrightarrow max_{\alpha}min_{w,b}L(\vec w, b, \vec \alpha)$$

Solve $min_{w, b}L(\vec w, b, \vec \alpha)$:

$$\frac{dL}{d\vec w}=\vec w - \sum \alpha_iy^{(i)}x^{(i)}=0$$

$$\Longleftrightarrow \vec w = \sum \alpha_iy^{(i)}x^{(i)}$$

$$\frac{dL}{db} = - \sum \alpha_iy^{(i)} = 0 (constrain)$$

So SVM dual:

$$max_{\vec \alpha} L(\vec \alpha) = \frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy^{(i)}y^{(j)}\vec x^{(i)T}\vec x^{(j)} - \sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy^{(i)}y^{(j)}\vec x^{(i)T}\vec x^{(j)} - \sum_{i=1}^{N}\alpha_iy^{(i)}b + \sum_{i=1}^{N}  \alpha$$


$$\;\;\;=\sum_{i=1}^{N}  \alpha-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy^{(i)}y^{(j)}\vec x^{(i)T}\vec x^{(j)}$$

$$s.t. \;\alpha \geq 0\;\;\sum_{i=1}^{N}\alpha_iy^{(i)}=0$$

Because there is a constrain in the obeject function, the classic gradient descent method can not be applied. The most popular algorithem for solving SVM dual is called [Sequential minimal optimization](https://en.wikipedia.org/wiki/Sequential_minimal_optimization). 
