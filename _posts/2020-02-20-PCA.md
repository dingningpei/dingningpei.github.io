---
layout: page
title: "PCA"
date: 2020-02-12 12:39:12 -0000

categories: 
- Machine Learning
- Math
---

# Object of PCA
$v= argmax_v \frac{1}{N}X^TX$

This means we need to maximum the variance for each x.

Tips: X is centrelized so mean of the x is 0

If we can make $\frac{1}{N}X^TX$ diagonalized, theoretical all covariances are removed and our goal will be achieved. 

# Why does eigenvector  maximum the variance?

$$XX^T = EDE^T$$
$$P = E^T$$
So P is Orthogonal matrix. 

$$\frac{1}{N}X^TX= \frac{1}{N}P(X^TX)P^T=\frac{1}{N}P(X^TX)P^T = \frac{1}{N}P(P^TDP)P^T = \frac{1}{N}D $$

Then $\frac{1}{N}X^TX$ is diagonalized.

# How does SVD fit in PCA?

$$XX^T = V \Sigma U^T (V \Sigma U^T)^T = V\Sigma U^TU\Sigma^T V^T = V\Sigma^2V^T$$
$$P = V^T$$
$$PXX^TP^T = PV\Sigma^2V^TP^T = \Sigma^2 $$
